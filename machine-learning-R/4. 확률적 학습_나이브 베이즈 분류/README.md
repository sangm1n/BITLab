나이브 베이즈의 이해
================
*이상민*

-----

## 나이브 베이즈 분류 (naive bayes)

확률은 0에서 1 사이의 숫자로 가용한 증거를 고려해 사건이 발생할 가능성을 표현한 것이다. 확률이 낮을수록 사건의 발생 가능성은
적어진다.  
베이지안 기법 기반의 분류기는 훈련 데이터를 활용해 특징 값이 제공하는 증거를 기반으로 결과가 관측될 확률을 계산한다. 분류기가
레이블이 없는 데이터에 적용될 때 관측될 확률을 이용해 가장 유력한 클래스를 예측한다. 베이지안 분류기는 다음과 같은 분야에서
사용된다.

  - 스팸 이메일 필터링과 같은 텍스트 분류
  - 컴퓨터 네트워크에서 침입이나 비정상 행위 탐지
  - 일련의 관찰된 증상에 대한 의학적 질병 진단

대다수의 머신러닝 알고리즘이 영향력이 약한 특징은 무시하지만, 베이지안 기법은 가용한 모든 증거를 활용해 예측을 절묘하게 바꾼다.

-----

### 베이지안 기법의 기본 개념

베이지안 확률의 이론은 사건에 대한 우도(가능성)는 복수 시행에서 즉시 이용할 수 있는 증거를 기반으로 추정해야만 한다는
아이디어에 뿌리를 두고 있다. 다음 표는 몇 가지 결과에 대한 사건과 시행을 보여준다.

| 사건        | 시행         |
| --------- | ---------- |
| 앞면의 결과    | 동전 던지기     |
| 비가 오는 날씨  | 하루         |
| 메시지가 스팸이다 | 받은 이메일 메시지 |

#### 확률의 이해

사건의 확률은 관측 데이터에서 사건이 발생한 시행 횟수를 전체 시행 횟수로 나눠서 추정한다. 확률을 나타내고자 **사건 A의
확률**을 의미하는 P(A) 형태의 표기를 사용한다. 시행을 하면 항상 어떠한 결과가 발생하기 때문에 가능한 모든 시행
결과의 확률은 항상 1로 합산되어야 한다. <br><br>

#### 결합 확률의 이해

A∩B 표기는 A와 B가 모두 발생하는 사건을 나타낸다. 이것을 계산하는 것은 두 사건의 결합 확률에 따라 달라진다. 두 사건이
완전히 관련이 없다면 **독립 사건**이라고 한다. 독립 사건이 동시에 발생할 수 없다는 뜻은 아니다. 따라서 사건 독립성은
단순히 한 사건의 결과를 아는 것만으로는 다른 사건의 결과에 대해 어떠한 정보도 제공하지 못한다는 것을 뜻한다.  
모든 사건이 독립이라면 다른 사건을 관측해 어떤 사건을 예측하는 것은 불가능하다. 달리 말하면 **종속 사건**이 예측 모델링의
기반이 된다. 독립 사건 A와 B에 대해 둘 다 발생할 확률은 P(A∩B) = P(A) × P(B) 처럼 계산할 수 있다.
하지만 종속 사건의 확률을 계산하는 것은 독립 사건의 확률을 계산하는 것보다 복잡하다. <br><br>

#### 베이즈 정리를 이용한 조건부 확률 계산

종속 사건 간의 관계는 베이즈 정리를 이용해 설명할 수 있다.

<p align=center>
<img src="formula/CodeCogsEqn.png">
</p>

P(B|A)는 ’사건 B가 발생한 경우 사건 A의 확률’로 읽는다. 이는 **조건부 확률**이라고 하는데, A의 확률이 사건 B가
발생한 경우에 종속적이기 때문이다.  
다음과 같은 공식은 전통적으로 베이즈 이론을 나타내는 방식이다.

<p align=center>
<img src="formula/CodeCogsEqn (1).png">
</p>

이 식에서 P(A)는 사전 확률, 즉 특정 사상이 일어나기 전의 확률이다. 이 식에서 P(B|A)는 우도, P(B)는 주변
우도라고 한다. 사전 확률과 우도를 토대로 베이즈 이론을 적용해 사후 확률 P(A|B)를 계산할 수 있다.

-----

### 나이브 베이즈 알고리즘

나이브 베이즈 알고리즘은 분류 문제에 베이즈 이론을 적용할 수 있는 단순한 방법을 정의해준다.

| 장점                    | 단점                                       |
| --------------------- | ---------------------------------------- |
| 간단하고 빠르며 매우 효율적       | 모든 특징이 동등하게 중요하고 독립이라는 가정이 잘못된 경우가 종종 있음 |
| 노이즈와 누락 데이터 잘 처리      | 수치 특징이 많은 데이터셋에는 이상적이지 않음                |
| 예측용 추정 확률을 쉽게 얻을 수 있음 | 추정된 확률이 예측된 클래스보다 덜 신뢰할만함                |

나이브 베이즈는 데이터셋의 모든 특징이 동등하게 중요하고 독립적이라고 가정한다. 이러한 가정은 대부분의 실제 응용에는 거의 맞지
않지만 나이브 베이즈는 여전히 잘 작동하고 정확하다.

#### 나이브 베이즈를 이용한 분류

특징이 추가되면 가능한 모든 사건 교집합에 대해 확률을 저장해야 하므로 엄청난 양의 메모리가 필요하다. 그러므로 나이브 베이즈가
사건 간의 독립을 가정한다는 사실을 활용하면 이 작업이 용이해진다. 특히 클래스 조건부 독립, 즉 해당 동일 특정 클래스에
대한 조건에 사건이 독립이라고 가정한다. 그렇게 되면 독립 사건의 확률실 P(A∩B) = P(A) × P(B)을 사용할 수
있다.

<p align=center>
<img src="formula/CodeCogsEqn (2).png">
</p>

#### 라플라스 추정량

나이브 베이즈 공식에서 확률은 체인으로 곱해졌기 때문에 0% 값은 사후 확률을 0으로 만들어 다른 모든 증거를 무효화하고 기각하게
만든다. 이러한 문제의 해결책은 라츨라스 추정량을 사용하는 것이다. 라플라스 추정량은 빈도표의 각 합계에 작은 숫자를 더하는데,
특징이 각 클래스에 대해 발생할 확률이 0이 되지 않도록 보장한다. 보통 라플라스 추정량은 1로 설정해서 데이터에 각 클래스
특징 조합이 최소 한 번은 나타나도록 보장한다. <br><br>

#### 나이브 베이즈에서 수치 특성 이용

나이브 베이즈는 행렬을 구성하는 각 특징이 범주형이어야 한다. 수치 특성은 값의 범주가 없어 위의 알고리즘은 수치 데이터에 직접
작동되지 않는다. 하지만 이것을 쉽게 해결할 수 있는 방법 중 하나는 수치 특징을 이산화하는 것이다. 간단히 빈(bin)이라고
하는 범주에 숫자를 넣는 것을 의미한다.  
유념해야 할 점은 수치 특징을 이산화하면 항상 정보가 손실된다는 점이다. 빈의 개수가 너무 적으면 중요한 추세가 모호해지고,
반대로 빈의 개수가 너무 많으면 알고리즘의 노이즈 데이터 민감도가 올라갈 수 있다.

