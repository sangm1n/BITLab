모델 성능 평가 (2)
================
*이상민*

-----

## 미래의 성능 예측

일부 R 머신러닝 패키지는 모델을 구축하는 과정에서 혼동 행렬과 성능 척도를 제공한다. 이 통계의 목적은 모델의 재치환 오류에
대한 통찰력을 제공하는 것이다. 재치환 오류란 모델이 훈련 데이터에서 직접 만들어졌음에도 불구하고 훈련 데이터를 부정확하게
에측할 때 발생하는 오류이다.

-----

### 홀드아웃 방법

주어진 데이터를 랜덤하게 훈련 데이터셋과 테스트 데이터셋으로 분할하는 절차를 **홀드아웃 방법** 이라고 한다. 일반적으로 데이터의
약 1/3은 테스트를 위해 유지되며, 2/3는 훈련을 위해 사용된다. 홀드아웃 방법이 미래 성능에 대한 실제 정확한 추정치를
얻으려면 테스트 데이터셋에 대한 성능이 모델에 영향을 미치지 않게 해야만 한다. 반복되는 테스트 결과를 바탕으로 최고의
모델을 선택하기 때문에 이 규칙은 자신도 모르게 위반하기 쉽다.  
이 문제를 피하려면 훈련 데이터셋과 테스트 데이터셋 외에 검증 데이터셋을 사용할 수 있도록 데이터를 나누는 것이 더 좋다. 검증
데이터셋은 선택된 모델을 반복하고 개선하는 데 사용된다. 훈련, 테스트, 검증 사이에 대표적인 분할은 각각 50%, 25%,
25%이다.  
1,000개의 데이터 행으로 된 credit을 예로 들면, runif() 함수를 사용해 1에서 1000까지 임의로 정렬된 행
벡터를 사용한다.

``` r
random_idx <- order(runif(1000))

credit_train <- credit[random_idx[1:500], ]
credit_validate <- credit[random_idx[501:750], ]
credit_test <- credit[random_idx[751:1000], ]
```

홀드아웃 샘플링의 문제 중 하나는 각 파티션에 어떤 클래스의 비율이 더 많거나 더 적을 수 있다는 점이다. 이 문제 발생의
가능성을 줄이고자 층별 랜덤 샘플링으로 불리는 기법을 사용할 수 있다. 층별 랜덤 샘플링은 랜덤 파티션이 일부
클래스가 작더라도 각 클래스 값을 전체 데이터셋에서와 거의 동일한 비율로 갖도록 보장한다. caret 패키지에서
제공하는 createDataPartition() 함수를 사용하면 되는데, 이 함수를 사용하려면 파티션에 포함될
인스턴스 비율을 명시하는 파라미터 p 이외에 클래스 값 벡터가 명시돼야만 한다.

``` r
library(caret)
in_train <- createDataPartition(credit$default, p=0.75, list=FALSE)
credit_train <- credit[in_train, ]
credit_test <- credit[-in_train, ]
```

층별 샘플링은 클래스를 골고루 배분하지만 다른 종류의 대표성은 보장하지 않는다. 일부 샘플은 너무 많거나 너무 적은 어려운
케이스, 쉽게 예측되는 케이스 또는 이상치를 가질 수 있다.  
반복 홀드아웃이라고 하는 기법은 무작위로 구성된 훈련 데이터셋의 문제를 완화하는 데 가끔씩 사용된다. 이 방법은 모델의 성능을
평가하고자 여러 랜덤 홀드아웃 샘플에서 평균 결과를 사용하는 홀드아웃 방법의 특별한 경우이다.

#### 교차 검증

반복 홀드아웃은 모델 성능을 추정하고자 산업 표준이 된 **k-폴드 교차 검증(k-fold CV,
Cross-Validation)** 이라고 하는 기법의 기반이다. 잠재적으로 같은 레코드가 한 번 이상 사용될 수 있는 반복 랜덤
샘플을 추출하는 대신 k-폴드 교차 검증은 데이터를 무작위 k개로 나눠 랜덤 파티션으로 완전히 분리한다. 일반적으로는
10-fold CV를 사용한다. 10 폴드의 각각에 대해 머신러닝 모델은 남은 90%의 데이터를 기반으로 구축된다. 이때 폴드에
해당하는 10% 샘플은 모델의 평가에 사용된다.

``` r
library(caret)
folds <- createFolds(credit$default, k = 10)
str(folds)
```

    ## List of 10
    ##  $ Fold01: int [1:100] 7 21 34 36 50 94 97 127 139 146 ...
    ##  $ Fold02: int [1:100] 15 54 56 57 68 72 83 89 96 99 ...
    ##  $ Fold03: int [1:100] 13 27 31 55 60 65 70 87 113 133 ...
    ##  $ Fold04: int [1:100] 5 6 19 39 47 73 74 82 98 107 ...
    ##  $ Fold05: int [1:100] 9 23 26 33 35 53 59 61 64 76 ...
    ##  $ Fold06: int [1:100] 1 12 25 29 30 37 52 58 62 124 ...
    ##  $ Fold07: int [1:100] 2 17 32 40 43 44 45 46 67 69 ...
    ##  $ Fold08: int [1:100] 4 11 16 18 20 38 41 42 79 90 ...
    ##  $ Fold09: int [1:100] 3 8 10 49 78 85 86 123 132 142 ...
    ##  $ Fold10: int [1:100] 14 22 24 28 48 51 63 66 75 84 ...

``` r
credit01_test <- credit[folds$Fold01, ]
credit01_train <- credit[-folds$Fold01, ]
```

전체 10-폴드 교차 검증을 수행하려면 이 단계는 총 10회 반복해야 한다. 먼저 모델을 구축하고 난 후 매번 모델의 성능을
계산한다. 마지막으로 성능 측정치를 평균해서 전체 성능을 구한다.

<br>

#### 부트스트랩 샘플링

k-폴드 교차 검증의 대안으로 약간 덜 보편적이지만 그래도 폭넓게 사용되는 부트스트랩 샘플링, 줄여서
**부트스트랩(bootstrap)** 으로 알려져 있다. 일반적으로 말하면 아주 큰 집합의 속성을 추정하고자
데이터의 랜덤 샘플을 사용하는 통계적 방법을 말한다.  
교차 검증은 데이터를 여러 파티션으로 나눌 때 각 예시가 파티션에 단 한 번만 나타나는 반면 부트스트랩에서는 복원 샘플링 과정을
거쳐 예시가 여러 번 선택될 수 있다. 부트스트랩 과정은 n개의 예시로 구성된 원래 데이터셋에서 한 개 이상의 새로운 훈련
데이터셋을 생성하며, 생성된 훈련 데이터셋 또한 n개의 예시를 포함하고 그중 일부는 반복된다. 그 후 대응되는 테스트
데이터셋은 훈련 데이터셋에 선택되지 않은 예시에서 구성된다.
